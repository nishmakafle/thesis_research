{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Concatenate, Input, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "import pywt\n",
    "import cv2\n",
    "import PIL.Image\n",
    "import PIL.ImageChops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForensicFeatureExtractor:\n",
    "    @staticmethod\n",
    "    def error_level_analysis(image_path, output_quality=90):\n",
    "        \"\"\"\n",
    "        Perform Error Level Analysis (ELA)\n",
    "\n",
    "        Args:\n",
    "            image_path (str): Path to input image\n",
    "            output_quality (int): JPEG compression quality\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Scaled ELA feature map\n",
    "        \"\"\"\n",
    "        original_image = PIL.Image.open(image_path)\n",
    "        temp_compressed_path = 'temp_compressed.jpg'\n",
    "        original_image.save(temp_compressed_path, 'JPEG', quality=output_quality)\n",
    "        compressed_image = PIL.Image.open(temp_compressed_path)\n",
    "        ela_image = PIL.ImageChops.difference(original_image, compressed_image)\n",
    "        ela_array = np.array(ela_image)\n",
    "        scaled_ela = ela_array * (255.0 / np.max(ela_array))\n",
    "        return scaled_ela\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def wavelet_noise_features(image, wavelet='db4', levels=3, expected_features=9):\n",
    "        \"\"\"\n",
    "        Extract noise features using wavelet transform.\n",
    "    \n",
    "        Args:\n",
    "            image (numpy.ndarray): Input image\n",
    "            wavelet (str): Wavelet type\n",
    "            levels (int): Decomposition levels\n",
    "            expected_features (int): Expected number of features for the model\n",
    "    \n",
    "        Returns:\n",
    "            numpy.ndarray: Noise feature vector with the specified number of features\n",
    "        \"\"\"\n",
    "        if len(image.shape) == 3:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "        coeffs = pywt.wavedec2(image, wavelet, level=levels)\n",
    "        noise_features = []\n",
    "        for i in range(1, levels + 1):\n",
    "            subbands = coeffs[i]\n",
    "            for sb in subbands:\n",
    "                noise_features.extend([\n",
    "                    np.mean(np.abs(sb)),\n",
    "                    np.std(sb),\n",
    "                    np.max(np.abs(sb))\n",
    "                ])\n",
    "        \n",
    "        if len(noise_features) > expected_features:\n",
    "            noise_features = noise_features[:expected_features]  # Truncate\n",
    "        elif len(noise_features) < expected_features:\n",
    "            noise_features.extend([0] * (expected_features - len(noise_features)))  # Pad with zeros\n",
    "        \n",
    "        return np.array(noise_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridForensicModel:\n",
    "    def __init__(self, input_shape=(224, 224, 3), num_classes=2):\n",
    "        \"\"\"\n",
    "        Create hybrid forensic detection model\n",
    "\n",
    "        Args:\n",
    "            input_shape (tuple): Input image shape\n",
    "            num_classes (int): Number of output classes\n",
    "        \"\"\"\n",
    "        base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "        image_input = Input(shape=input_shape, name='image_input')\n",
    "        image_features = base_model(image_input)\n",
    "        image_features = GlobalAveragePooling2D()(image_features)\n",
    "\n",
    "        ela_input = Input(shape=(224, 224, 1), name='ela_input')\n",
    "        ela_conv = tf.keras.layers.Conv2D(16, (3, 3), activation='relu')(ela_input)\n",
    "        ela_pool = tf.keras.layers.MaxPooling2D((2, 2))(ela_conv)\n",
    "        ela_features = GlobalAveragePooling2D()(ela_pool)\n",
    "\n",
    "        noise_input = Input(shape=(9,), name='noise_input')\n",
    "\n",
    "        combined_features = Concatenate()([\n",
    "            image_features,\n",
    "            ela_features,\n",
    "            noise_input\n",
    "        ])\n",
    "        \n",
    "        x = Dense(256, activation='relu')(combined_features)\n",
    "        x = Dropout(0.5)(x)\n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        output = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "        self.model = Model(inputs=[image_input, ela_input, noise_input], outputs=output)\n",
    "        self.model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "                           loss='categorical_crossentropy',\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "    def prepare_input(self, image_path):\n",
    "        \"\"\"\n",
    "        Prepare input for the model.\n",
    "    \n",
    "        Args:\n",
    "            image_path (str): Path to input image.\n",
    "    \n",
    "        Returns:\n",
    "            tuple: Preprocessed image, ELA map, and noise features.\n",
    "        \"\"\"\n",
    "        # Read and preprocess the image\n",
    "        image = cv2.imread(image_path)\n",
    "        processed_image = cv2.resize(image, (224, 224))\n",
    "        processed_image = preprocess_input(processed_image)\n",
    "    \n",
    "        # Generate and preprocess the ELA map\n",
    "        ela_map = ForensicFeatureExtractor.error_level_analysis(image_path)\n",
    "        ela_map_resized = cv2.resize(ela_map, (224, 224))\n",
    "        ela_map_normalized = np.clip(ela_map_resized / 255.0, 0, 1)  # Normalize to [0, 1]\n",
    "        ela_map_normalized = (ela_map_normalized * 255).astype(np.uint8)  # Convert to uint8\n",
    "        \n",
    "        # Convert to grayscale if needed\n",
    "        if ela_map_normalized.ndim == 3 and ela_map_normalized.shape[-1] == 3:\n",
    "            ela_map_normalized = cv2.cvtColor(ela_map_normalized, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Expand dimensions\n",
    "        ela_map_expanded = np.expand_dims(ela_map_normalized, axis=-1)  # Add channel dimension\n",
    "          # Add channel dimension\n",
    "    \n",
    "        # Extract noise features\n",
    "        noise_features = ForensicFeatureExtractor.wavelet_noise_features(image)\n",
    "    \n",
    "        return processed_image, ela_map_expanded, noise_features\n",
    "\n",
    "\n",
    "    def train(self, image_paths, labels, epochs=10, batch_size=32):\n",
    "        \"\"\"\n",
    "        Train the hybrid forensic model\n",
    "\n",
    "        Args:\n",
    "            image_paths (list): Paths to training images\n",
    "            labels (numpy.ndarray): One-hot encoded labels\n",
    "            epochs (int): Number of training epochs\n",
    "            batch_size (int): Training batch size\n",
    "        \"\"\"\n",
    "        X_images, X_ela, X_noise = [], [], []\n",
    "        for img_path in image_paths:\n",
    "            proc_img, proc_ela, proc_noise = self.prepare_input(img_path)\n",
    "            X_images.append(proc_img)\n",
    "            X_ela.append(proc_ela)\n",
    "            X_noise.append(proc_noise)\n",
    "\n",
    "        X_images = np.array(X_images)\n",
    "        X_ela = np.array(X_ela)\n",
    "        X_noise = np.array(X_noise)\n",
    "\n",
    "        history = self.model.fit(\n",
    "            [X_images, X_ela, X_noise],\n",
    "            labels,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_split=0.2\n",
    "        )\n",
    "        return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(genuine_dir, forged_dir):\n",
    "    \"\"\"\n",
    "    Load dataset from separate directories for genuine and forged images.\n",
    "\n",
    "    Args:\n",
    "        genuine_dir (str): Path to genuine images directory\n",
    "        forged_dir (str): Path to forged images directory\n",
    "\n",
    "    Returns:\n",
    "        tuple: Image paths and labels\n",
    "    \"\"\"\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "\n",
    "    # Process genuine images\n",
    "    for filename in os.listdir(genuine_dir):\n",
    "        if filename.endswith(('.jpg', '.png', '.tif', '.tiff')):\n",
    "            image_paths.append(os.path.join(genuine_dir, filename))\n",
    "            labels.append([1, 0])  # Label for genuine images\n",
    "\n",
    "    # Process forged images\n",
    "    for filename in os.listdir(forged_dir):\n",
    "        if filename.endswith(('.jpg', '.png', '.tif', '.tiff')):\n",
    "            image_paths.append(os.path.join(forged_dir, filename))\n",
    "            labels.append([0, 1])  # Label for forged images\n",
    "\n",
    "    return image_paths, np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder_path = './CASIA22'\n",
    "genuine_image_dir = f'{image_folder_path}/Au'\n",
    "forged_image_dir = f'{image_folder_path}/Tp'\n",
    "\n",
    "# image_paths, labels = load_dataset(genuine_image_dir, forged_image_dir)\n",
    "\n",
    "# hybrid_model = HybridForensicModel()\n",
    "# history = hybrid_model.train(image_paths, labels, epochs=25)\n",
    "\n",
    "# test_image_path = '/path/to/test/image.jpg'  # Replace with a test image path\n",
    "# prediction = hybrid_model.predict(test_image_path)\n",
    "# print(\"Forgery Probability:\", prediction[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 765ms/step - accuracy: 0.4994 - loss: 19.6855 - val_accuracy: 0.6015 - val_loss: 2.5739\n",
      "Epoch 2/10\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 751ms/step - accuracy: 0.5251 - loss: 7.0123 - val_accuracy: 0.6085 - val_loss: 0.7062\n",
      "Epoch 3/10\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 1s/step - accuracy: 0.5112 - loss: 3.3293 - val_accuracy: 0.5960 - val_loss: 0.7079\n",
      "Epoch 4/10\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 843ms/step - accuracy: 0.5435 - loss: 1.7854 - val_accuracy: 0.5940 - val_loss: 0.6765\n",
      "Epoch 5/10\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 828ms/step - accuracy: 0.5547 - loss: 1.2938 - val_accuracy: 0.5930 - val_loss: 0.6715\n",
      "Epoch 6/10\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 831ms/step - accuracy: 0.5741 - loss: 0.9858 - val_accuracy: 0.5935 - val_loss: 0.6647\n",
      "Epoch 7/10\n",
      "\u001b[1m130/252\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:32\u001b[0m 760ms/step - accuracy: 0.5795 - loss: 0.8485"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 134\u001b[0m\n\u001b[1;32m    131\u001b[0m train_paths, test_paths, train_labels, test_labels \u001b[38;5;241m=\u001b[39m evaluator\u001b[38;5;241m.\u001b[39mprepare_dataset(image_paths, labels, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mhybrid_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Plot training history\u001b[39;00m\n\u001b[1;32m    137\u001b[0m evaluator\u001b[38;5;241m.\u001b[39mplot_training_history(history)\n",
      "Cell \u001b[0;32mIn[4], line 98\u001b[0m, in \u001b[0;36mHybridForensicModel.train\u001b[0;34m(self, image_paths, labels, epochs, batch_size)\u001b[0m\n\u001b[1;32m     95\u001b[0m X_ela \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(X_ela)\n\u001b[1;32m     96\u001b[0m X_noise \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(X_noise)\n\u001b[0;32m---> 98\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_ela\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_noise\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\n\u001b[1;32m    104\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m history\n",
      "File \u001b[0;32m~/Documents/SoftwareProjects/Thesis/venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Documents/SoftwareProjects/Thesis/venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:318\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    317\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 318\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[1;32m    320\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
      "File \u001b[0;32m~/Documents/SoftwareProjects/Thesis/venv/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Documents/SoftwareProjects/Thesis/venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/Documents/SoftwareProjects/Thesis/venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/SoftwareProjects/Thesis/venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/SoftwareProjects/Thesis/venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/Documents/SoftwareProjects/Thesis/venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/Documents/SoftwareProjects/Thesis/venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/SoftwareProjects/Thesis/venv/lib/python3.12/site-packages/tensorflow/python/eager/context.py:1552\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1552\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1553\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1560\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1561\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1562\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1566\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1567\u001b[0m   )\n",
      "File \u001b[0;32m~/Documents/SoftwareProjects/Thesis/venv/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "class HybridForensicModelEvaluator:\n",
    "    def __init__(self, model):\n",
    "        \"\"\"\n",
    "        Initialize the evaluator with a trained HybridForensicModel instance.\n",
    "\n",
    "        Args:\n",
    "            model (HybridForensicModel): Trained model to evaluate.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "\n",
    "    def prepare_dataset(self, image_paths, labels, test_size=0.2, random_state=42):\n",
    "        \"\"\"\n",
    "        Split data into training and testing sets.\n",
    "\n",
    "        Args:\n",
    "            image_paths (list): List of image file paths.\n",
    "            labels (numpy.ndarray): One-hot encoded labels.\n",
    "            test_size (float): Proportion of data to use for testing.\n",
    "            random_state (int): Random seed for reproducibility.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Train and test splits for image paths and labels.\n",
    "        \"\"\"\n",
    "        train_paths, test_paths, train_labels, test_labels = train_test_split(\n",
    "            image_paths, labels, test_size=test_size, random_state=random_state\n",
    "        )\n",
    "        return train_paths, test_paths, train_labels, test_labels\n",
    "\n",
    "    def evaluate(self, image_paths, labels):\n",
    "        \"\"\"\n",
    "        Evaluate the model on a dataset.\n",
    "\n",
    "        Args:\n",
    "            image_paths (list): List of image file paths.\n",
    "            labels (numpy.ndarray): One-hot encoded true labels.\n",
    "\n",
    "        Returns:\n",
    "            dict: Evaluation metrics (accuracy, precision, recall, F1-score).\n",
    "        \"\"\"\n",
    "        X_images, X_ela, X_noise = [], [], []\n",
    "\n",
    "        # Prepare inputs\n",
    "        for img_path in image_paths:\n",
    "            proc_img, proc_ela, proc_noise = self.model.prepare_input(img_path)\n",
    "            X_images.append(proc_img)\n",
    "            X_ela.append(proc_ela)\n",
    "            X_noise.append(proc_noise)\n",
    "\n",
    "        X_images = np.array(X_images)\n",
    "        X_ela = np.array(X_ela)\n",
    "        X_noise = np.array(X_noise)\n",
    "\n",
    "        # Get predictions\n",
    "        predictions = self.model.model.predict([X_images, X_ela, X_noise])\n",
    "        predicted_classes = np.argmax(predictions, axis=1)\n",
    "        true_classes = np.argmax(labels, axis=1)\n",
    "\n",
    "        # Compute metrics\n",
    "        accuracy = accuracy_score(true_classes, predicted_classes)\n",
    "        report = classification_report(true_classes, predicted_classes, output_dict=True)\n",
    "\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(\"Classification Report:\")\n",
    "        print(classification_report(true_classes, predicted_classes))\n",
    "\n",
    "        # Confusion matrix\n",
    "        conf_matrix = confusion_matrix(true_classes, predicted_classes)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(true_classes), yticklabels=np.unique(true_classes))\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.xlabel('Predicted Labels')\n",
    "        plt.ylabel('True Labels')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": report[\"weighted avg\"][\"precision\"],\n",
    "            \"recall\": report[\"weighted avg\"][\"recall\"],\n",
    "            \"f1_score\": report[\"weighted avg\"][\"f1-score\"]\n",
    "        }\n",
    "    \n",
    "    def plot_training_history(self, history):\n",
    "        \"\"\"\n",
    "        Plot training and validation accuracy and loss.\n",
    "\n",
    "        Args:\n",
    "            history (History): Training history object from model.fit().\n",
    "        \"\"\"\n",
    "        # Plot accuracy\n",
    "        plt.figure(figsize=(12, 6))\n",
    "\n",
    "        # Accuracy plot\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.title('Training and Validation Accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "\n",
    "        # Loss plot\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history.history['loss'], label='Training Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "# Example Usage\n",
    "# Assuming `image_paths` is a list of all image paths and `labels` is a numpy array of one-hot encoded labels.\n",
    "image_paths, labels = load_dataset(genuine_image_dir, forged_image_dir)\n",
    "\n",
    "# Instantiate your model (ensure it's already trained)\n",
    "hybrid_model = HybridForensicModel()\n",
    "\n",
    "# Instantiate the evaluator\n",
    "evaluator = HybridForensicModelEvaluator(hybrid_model)\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_paths, test_paths, train_labels, test_labels = evaluator.prepare_dataset(image_paths, labels, test_size=0.2)\n",
    "\n",
    "# Train the model\n",
    "history = hybrid_model.train(train_paths, train_labels, epochs=10, batch_size=32)\n",
    "\n",
    "# Plot training history\n",
    "evaluator.plot_training_history(history)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluation_metrics = evaluator.evaluate(test_paths, test_labels)\n",
    "print(\"Test Evaluation Metrics:\", evaluation_metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
